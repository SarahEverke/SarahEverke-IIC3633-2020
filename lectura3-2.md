## Crítica: *Performance of Recommender Algorithms on Top-N Recommendation Tasks*

Cremonesi et al. evalúan el rendimiento de diversos algoritmos de recomendación al sugerir top-N elementos. A cada elemento *i* del set de testeo con un *rating* igual a 5, lo unen a un conjunto con 1000 items desconocidos por el usuario. Luego, le calulan el *rating* a cada uno y los ordenan. Escogen los N elementos y si el elemto *i* está en esa selección se considera un *hit* sino un *miss*. Las métricas usadas son *recall* que relaciona la cantidad de *hits* con el número de elementos del set de testeo y *precision*. A continuación, comentaré aspectos que considero interesantes y las dudas que tuve luego de leer el texto.

Primero, este *paper* es útil porque compara los rendimientos de diversos algoritmos en la tarea de recomendar los mejores N elementos. Lo que permite que uno pueda guiarse con estos resultados para seleccionar un sistema recomendador con ese objetivo. No obstante, les faltó indicar el hiperparámetro (del tamaño de la vecindad) para los modelos de filtrado colaborativo basado en vecindades como NNCosNgbr. Este parámetro sería importante en caso de replicar las pruebas. 

Segundo, considero positivo que hayan usado dos sets de datos: MovieLens y Netflix, ya que esto le entrega una mayor validez a los resultados.

Tercero, me llamó la atención cómo afecta la distribución de los datos en el rendimiento de los algoritmos. En las Figuras 2.a y 3.a se observan cómo se comportan con todos los datos y al excluir a los más populares. Por ejemplo, TopPop, un algoritmo no personalizado, claramente tiene un mejor *recall* cuando se consideran todos los datos. Esto confirma que antes de definir qué algoritmo se utilizará hay que estudiar las distribuciones de los datos. Además, creo que la idea de no considerar a los items más populares podría ayudar a generar recomendaciones más novedosas dado que probablemente los elementos que son más populares ya son conocidos por el usuario. 

Tercero, es interesante que el algoritmo PureSVD realizara mejores predicciones que SVD++, que es un modelo con mejor precisión en términos de RMSE. Esto señala que las métricas de precisión como RMSE y MAE tienen limitaciones como no considerar los items que nunca han sido evaluados. Por tanto, cuando uno quiera escoger un algoritmo de recomendación debe determinar un objetivo y según este seleccionar el más apropiado. 

Finalmente, este *paper* es de una gran utilidad porque muestra el desempeño de algoritmos conocidos al seleccionar los N mejores elementos. Aunque, faltaría analizar cómo evaluar a los algoritmos de datos implícitos porque las métricas usadas se basan en que el elemento que debe estar entre los *top-N* debe tener un  *rating* igual a 5 (información explícita).